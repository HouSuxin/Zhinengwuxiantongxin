{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用DNN拟合非线性函数\\n\n",
    "在本Notebook中，我们使用一个简单的具有一个隐藏层的全连接神经网络（DNN）来拟合函数 $y=\\\\sin x$ 。该模型的隐藏层使用ReLU作为激活函数，并选择均方误差（MSE）作为回归任务的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# 引入必要的包\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# 第一步：生成输入数据\n",
    "# 在这里，我们生成区间 [-2π, 2π] 中的x值，并对应生成y值为 sin(x)\n",
    "x = np.linspace(-2 * np.pi, 2 * np.pi, 1000).reshape(-1, 1)  # 在区间内生成1000个点\n",
    "y = np.sin(x)  # 真实函数为sin(x)\n",
    "\n",
    "# 输出生成的数据以便检查\n",
    "print(\"输入数据 x 的前5个值：\\n\", x[:5])\n",
    "print(\"目标函数 y=sin(x) 的前5个值：\\n\", y[:5])\n",
    "\n",
    "# 将numpy数组转换为PyTorch的张量\n",
    "x_tensor = torch.Tensor(x)\n",
    "y_tensor = torch.Tensor(y)\n",
    "\n",
    "# 输出转换后的张量数据以便检查\n",
    "print(\"转换后的输入张量 x_tensor 的形状：\", x_tensor.shape)\n",
    "print(\"转换后的目标张量 y_tensor 的形状：\", y_tensor.shape)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 第二步：定义DNN模型\n",
    "# 这个模型包含一个隐藏层，具有10个神经元，并使用ReLU激活函数\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)  # 输入层（1维输入）到隐藏层（10个神经元）\n",
    "        self.relu = nn.ReLU()        # 激活函数：ReLU\n",
    "        self.fc2 = nn.Linear(10, 1)  # 隐藏层到输出层（1维输出）\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播：输入 -> 隐藏层 -> 激活函数 -> 输出层\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)  # 在第一层之后应用ReLU激活函数\n",
    "        out = self.fc2(out)   # 输出层为线性层\n",
    "        return out\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 第三步：初始化模型、损失函数和优化器\n",
    "model = DNN()  # 实例化DNN模型\n",
    "criterion = nn.MSELoss()  # 损失函数：均方误差（MSE）用于回归问题\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # 优化器：Adam算法\n",
    "\n",
    "# 输出初始化模型结构\n",
    "print(\"初始化模型结构：\\n\", model)\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 第四步：训练模型\n",
    "num_epochs = 1000  # 设置训练轮数为1000\n",
    "losses = []  # 用于存储每个epoch的损失值\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 前向传播：通过模型将x_tensor输入，计算预测的y\n",
    "    outputs = model(x_tensor)\n",
    "    loss = criterion(outputs, y_tensor)  # 计算预测的y与真实y之间的损失\n",
    "    \n",
    "    # 反向传播及优化\n",
    "    optimizer.zero_grad()  # 清零当前batch的梯度\n",
    "    loss.backward()        # 进行反向传播以计算梯度\n",
    "    optimizer.step()       # 使用优化器更新模型参数\n",
    "\n",
    "    # 存储损失值\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # 每100个epoch打印一次当前的损失值\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        # 输出模型预测值的前5个结果\n",
    "        print(\"模型预测的前5个值：\\n\", outputs[:5].detach().numpy())\n",
    "        print(\"真实的目标值前5个值：\\n\", y[:5])\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 第五步：绘制损失函数随时间的变化\n",
    "# 该图展示了随着模型训练，损失如何下降\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('损失函数随训练过程的变化')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 第六步：可视化拟合结果\n",
    "# 训练后，我们使用模型预测y的值，并将其与真实的sin(x)值绘制在一起\n",
    "with torch.no_grad():  # 禁用梯度计算，进入评估模式\n",
    "    predicted = model(x_tensor).numpy()  # 获取模型预测的y值\n",
    "\n",
    "# 绘制真实函数和模型预测函数\n",
    "plt.plot(x, y, label='真实函数 (sin x)')  # 真实sin(x)曲线\n",
    "plt.plot(x, predicted, label='DNN拟合函数')  # DNN预测的曲线\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
